{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evalutes some of the perforamnce of our pipeline in pred_model_implementation. functions were written, performed, and analyzed below. This can be useful for our presenttation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, certifi\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import tqdm as tqdm\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data and making df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "df1 = pd.read_parquet(\"lyric_embeddings/librosa_shard_0.parquet\")\n",
    "df2 = pd.read_parquet(\"lyric_embeddings/librosa_shard_1.parquet\")\n",
    "df3 = pd.read_parquet(\"lyric_embeddings/librosa_shard_2.parquet\")\n",
    "df4 = pd.read_parquet(\"lyric_embeddings/librosa_shard_3.parquet\")\n",
    "df5 = pd.read_parquet(\"lyric_embeddings/librosa_shard_4.parquet\")\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built with 20740 vectors.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "emb_list = [np.asarray(x, dtype=\"float32\") for x in df[\"lyrics_embedding\"].values]\n",
    "emb_matrix = np.stack(emb_list, axis=0)\n",
    "dimension = emb_matrix.shape[1]   # e.g., 768\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(emb_matrix.astype(\"float32\"))\n",
    "\n",
    "print(\"FAISS index built with\", index.ntotal, \"vectors.\")\n",
    "\n",
    "def retrieve_similar_songs(query_embedding: np.ndarray, k: int = 5) -> List[Dict[str, Any]]:\n",
    "    # Ensure query is 2D array (1, 384) for FAISS\n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "\n",
    "    # 1. Search the index\n",
    "    # Note: We search the 'index' object we created in the previous cell\n",
    "    D, I = index.search(query_vector, k)\n",
    "\n",
    "    neighbors = []\n",
    "    # I[0] contains the IDs, D[0] contains the distances\n",
    "    for idx, dist in zip(I[0], D[0]):\n",
    "        if idx != -1:\n",
    "            neighbors.append({\n",
    "                \"index\": int(idx),\n",
    "                \"similarity\": float(dist) # In L2, lower is better.\n",
    "            })\n",
    "\n",
    "    return neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# ---------- Cleaning helper for queries ----------\n",
    "\n",
    "def clean_lyrics_for_query(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple lyric cleaner for user queries.\n",
    "\n",
    "    • Lowercases text\n",
    "    • Removes [chorus], [verse 1], etc.\n",
    "    • Flattens newlines / \\n into spaces\n",
    "    • Strips things like (prod. xxx), (remix)\n",
    "    • Drops repeat markers like x2, x3\n",
    "    • Keeps letters (any language), digits, spaces, apostrophes\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove headers like [chorus], [verse 1], etc.\n",
    "    text = re.sub(r\"\\[.*?\\]\", \" \", text)\n",
    "\n",
    "    # handle real and escaped newlines\n",
    "    text = text.replace(\"\\\\n\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "    # remove (prod. ...), (remix ...)\n",
    "    text = re.sub(r\"\\(.*?prod.*?\\)\", \" \", text)\n",
    "    text = re.sub(r\"\\(.*?remix.*?\\)\", \" \", text)\n",
    "\n",
    "    # remove x2, x3, etc.\n",
    "    text = re.sub(r\"\\bx\\d+\\b\", \" \", text)\n",
    "\n",
    "    # keep letters (any language), numbers, spaces, apostrophes\n",
    "    chars = []\n",
    "    for ch in text:\n",
    "        cat = unicodedata.category(ch)\n",
    "        if cat.startswith(\"L\") or cat.startswith(\"N\") or ch in [\" \", \"'\", \"’\"]:\n",
    "            chars.append(ch)\n",
    "\n",
    "    text = \"\".join(chars)\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ---------- REAL embedding function ----------\n",
    "\n",
    "def embed_lyrics(text: str) -> np.ndarray:\n",
    "    cleaned = clean_lyrics_for_query(text)\n",
    "\n",
    "    # Encode returns a list of vectors when input is a list\n",
    "    emb = model.encode(\n",
    "        [cleaned],                    # IMPORTANT: wrap in list\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # emb has shape (1, D) → extract row 0\n",
    "    vec = emb[0]\n",
    "\n",
    "    # Ensure float32 1D\n",
    "    vec = np.asarray(vec, dtype=\"float32\").reshape(-1)\n",
    "\n",
    "    return vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>query_title</th>\n",
       "      <th>query_artist</th>\n",
       "      <th>track_genre</th>\n",
       "      <th>popularity</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>preview_url</th>\n",
       "      <th>track_id</th>\n",
       "      <th>...</th>\n",
       "      <th>spectral_contrast_6</th>\n",
       "      <th>spectral_contrast_7</th>\n",
       "      <th>tonnetz_1</th>\n",
       "      <th>tonnetz_2</th>\n",
       "      <th>tonnetz_3</th>\n",
       "      <th>tonnetz_4</th>\n",
       "      <th>tonnetz_5</th>\n",
       "      <th>tonnetz_6</th>\n",
       "      <th>lyrics_clean</th>\n",
       "      <th>lyrics_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4845</td>\n",
       "      <td>State of Mind</td>\n",
       "      <td>Scooter</td>\n",
       "      <td>state of mind</td>\n",
       "      <td>scooter</td>\n",
       "      <td>happy</td>\n",
       "      <td>24.0</td>\n",
       "      <td>The world seems not the same...\\n\\nIntroducing...</td>\n",
       "      <td>https://audio-ssl.itunes.apple.com/itunes-asse...</td>\n",
       "      <td>1692327616</td>\n",
       "      <td>...</td>\n",
       "      <td>18.328021</td>\n",
       "      <td>39.053367</td>\n",
       "      <td>0.197966</td>\n",
       "      <td>-0.116721</td>\n",
       "      <td>0.142559</td>\n",
       "      <td>-0.069539</td>\n",
       "      <td>-0.044986</td>\n",
       "      <td>-0.047523</td>\n",
       "      <td>the world seems not the same introducing twist...</td>\n",
       "      <td>[0.07519827783107758, -0.023364899680018425, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462</td>\n",
       "      <td>Reptilia</td>\n",
       "      <td>The Strokes</td>\n",
       "      <td>reptilia</td>\n",
       "      <td>the strokes</td>\n",
       "      <td>alt-rock</td>\n",
       "      <td>75.0</td>\n",
       "      <td>[Verse 1]\\nHe seemed impressed by the way you ...</td>\n",
       "      <td>https://audio-ssl.itunes.apple.com/itunes-asse...</td>\n",
       "      <td>302987569</td>\n",
       "      <td>...</td>\n",
       "      <td>17.382681</td>\n",
       "      <td>39.012014</td>\n",
       "      <td>0.078138</td>\n",
       "      <td>-0.077754</td>\n",
       "      <td>0.063345</td>\n",
       "      <td>0.036541</td>\n",
       "      <td>-0.011976</td>\n",
       "      <td>-0.014041</td>\n",
       "      <td>he seemed impressed by the way you came in tel...</td>\n",
       "      <td>[-0.08670999109745026, -0.025700576603412628, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16017</td>\n",
       "      <td>None Of My Business</td>\n",
       "      <td>Cher Lloyd</td>\n",
       "      <td>none of my business</td>\n",
       "      <td>cher lloyd</td>\n",
       "      <td>electro</td>\n",
       "      <td>64.0</td>\n",
       "      <td>[Chorus]\\nDamn, I heard that you and her been ...</td>\n",
       "      <td>https://audio-ssl.itunes.apple.com/itunes-asse...</td>\n",
       "      <td>1438630505</td>\n",
       "      <td>...</td>\n",
       "      <td>18.248683</td>\n",
       "      <td>39.966514</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>-0.092766</td>\n",
       "      <td>-0.056323</td>\n",
       "      <td>-0.004173</td>\n",
       "      <td>-0.014388</td>\n",
       "      <td>damn i heard that you and her been having prob...</td>\n",
       "      <td>[0.01792941242456436, 0.001567921251989901, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9478</td>\n",
       "      <td>Trouble Sleeping</td>\n",
       "      <td>The Perishers</td>\n",
       "      <td>trouble sleeping</td>\n",
       "      <td>the perishers</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>48.0</td>\n",
       "      <td>I'm having trouble sleeping\\nYou're jumping in...</td>\n",
       "      <td>https://audio-ssl.itunes.apple.com/itunes-asse...</td>\n",
       "      <td>89335271</td>\n",
       "      <td>...</td>\n",
       "      <td>16.969837</td>\n",
       "      <td>28.947224</td>\n",
       "      <td>-0.118755</td>\n",
       "      <td>0.195544</td>\n",
       "      <td>0.025169</td>\n",
       "      <td>-0.130705</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>i'm having trouble sleeping you're jumping in ...</td>\n",
       "      <td>[0.012034112587571144, -0.0008498362149111927,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2822</td>\n",
       "      <td>Shot in the Dark</td>\n",
       "      <td>Ozzy Osbourne</td>\n",
       "      <td>shot in the dark</td>\n",
       "      <td>ozzy osbourne</td>\n",
       "      <td>hard-rock</td>\n",
       "      <td>65.0</td>\n",
       "      <td>[Verse 1]\\nOut on the streets I'm stalking the...</td>\n",
       "      <td>https://audio-ssl.itunes.apple.com/itunes-asse...</td>\n",
       "      <td>158711416</td>\n",
       "      <td>...</td>\n",
       "      <td>17.184653</td>\n",
       "      <td>35.540522</td>\n",
       "      <td>-0.113671</td>\n",
       "      <td>0.023209</td>\n",
       "      <td>-0.029743</td>\n",
       "      <td>-0.051142</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>-0.011837</td>\n",
       "      <td>out on the streets i'm stalking the night i ca...</td>\n",
       "      <td>[-0.05440174415707588, 0.0212415661662817, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id                title         artist          query_title  \\\n",
       "0     4845        State of Mind        Scooter        state of mind   \n",
       "1      462             Reptilia    The Strokes             reptilia   \n",
       "2    16017  None Of My Business     Cher Lloyd  none of my business   \n",
       "3     9478     Trouble Sleeping  The Perishers     trouble sleeping   \n",
       "4     2822     Shot in the Dark  Ozzy Osbourne     shot in the dark   \n",
       "\n",
       "    query_artist track_genre  popularity  \\\n",
       "0        scooter       happy        24.0   \n",
       "1    the strokes    alt-rock        75.0   \n",
       "2     cher lloyd     electro        64.0   \n",
       "3  the perishers    acoustic        48.0   \n",
       "4  ozzy osbourne   hard-rock        65.0   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  The world seems not the same...\\n\\nIntroducing...   \n",
       "1  [Verse 1]\\nHe seemed impressed by the way you ...   \n",
       "2  [Chorus]\\nDamn, I heard that you and her been ...   \n",
       "3  I'm having trouble sleeping\\nYou're jumping in...   \n",
       "4  [Verse 1]\\nOut on the streets I'm stalking the...   \n",
       "\n",
       "                                         preview_url    track_id  ...  \\\n",
       "0  https://audio-ssl.itunes.apple.com/itunes-asse...  1692327616  ...   \n",
       "1  https://audio-ssl.itunes.apple.com/itunes-asse...   302987569  ...   \n",
       "2  https://audio-ssl.itunes.apple.com/itunes-asse...  1438630505  ...   \n",
       "3  https://audio-ssl.itunes.apple.com/itunes-asse...    89335271  ...   \n",
       "4  https://audio-ssl.itunes.apple.com/itunes-asse...   158711416  ...   \n",
       "\n",
       "  spectral_contrast_6  spectral_contrast_7 tonnetz_1  tonnetz_2  tonnetz_3  \\\n",
       "0           18.328021            39.053367  0.197966  -0.116721   0.142559   \n",
       "1           17.382681            39.012014  0.078138  -0.077754   0.063345   \n",
       "2           18.248683            39.966514  0.013912   0.172900  -0.092766   \n",
       "3           16.969837            28.947224 -0.118755   0.195544   0.025169   \n",
       "4           17.184653            35.540522 -0.113671   0.023209  -0.029743   \n",
       "\n",
       "   tonnetz_4  tonnetz_5  tonnetz_6  \\\n",
       "0  -0.069539  -0.044986  -0.047523   \n",
       "1   0.036541  -0.011976  -0.014041   \n",
       "2  -0.056323  -0.004173  -0.014388   \n",
       "3  -0.130705   0.024176   0.005865   \n",
       "4  -0.051142   0.003486  -0.011837   \n",
       "\n",
       "                                        lyrics_clean  \\\n",
       "0  the world seems not the same introducing twist...   \n",
       "1  he seemed impressed by the way you came in tel...   \n",
       "2  damn i heard that you and her been having prob...   \n",
       "3  i'm having trouble sleeping you're jumping in ...   \n",
       "4  out on the streets i'm stalking the night i ca...   \n",
       "\n",
       "                                    lyrics_embedding  \n",
       "0  [0.07519827783107758, -0.023364899680018425, -...  \n",
       "1  [-0.08670999109745026, -0.025700576603412628, ...  \n",
       "2  [0.01792941242456436, 0.001567921251989901, 0....  \n",
       "3  [0.012034112587571144, -0.0008498362149111927,...  \n",
       "4  [-0.05440174415707588, 0.0212415661662817, -0....  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_feature_cols = df.columns[df.columns.get_loc(\"duration\") : df.columns.get_loc(\"tonnetz_6\") + 1].tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_vector(\n",
    "    target_embedding: np.ndarray,\n",
    "    neighbors: List[Dict[str, Any]],\n",
    "    audio_feature_cols: List[str],\n",
    "    k: int = 3\n",
    ") -> np.ndarray:\n",
    "\n",
    "    vec = []\n",
    "\n",
    "    target_embedding = np.asarray(target_embedding, dtype=\"float32\").reshape(-1)\n",
    "    EMB_DIM = target_embedding.shape[0]\n",
    "    vec.extend(target_embedding.tolist())\n",
    "\n",
    "    AUDIO_DIM = len(audio_feature_cols)\n",
    "    NEIGHBOR_BLOCK = EMB_DIM + 2 + AUDIO_DIM  # emb + similarity + popularity + audio to ensure they all the same size\n",
    "\n",
    "    for i in range(k):\n",
    "        if i < len(neighbors):\n",
    "            nb = neighbors[i]\n",
    "\n",
    "            # neighbor embedding\n",
    "            nb_emb = df.iloc[nb[\"index\"]][\"lyrics_embedding\"]\n",
    "            nb_emb = np.asarray(nb_emb, dtype=\"float32\").reshape(-1)\n",
    "\n",
    "            # fill nans if neighbros dont exist\n",
    "            if nb_emb.shape[0] != EMB_DIM:\n",
    "                fixed_emb = np.full(EMB_DIM, np.nan, dtype=\"float32\")\n",
    "                fixed_emb[:min(EMB_DIM, len(nb_emb))] = nb_emb[:EMB_DIM]\n",
    "                nb_emb = fixed_emb\n",
    "\n",
    "            vec.extend(nb_emb.tolist())\n",
    "\n",
    "            # similarity\n",
    "            sim = nb.get(\"similarity\", np.nan)\n",
    "            vec.append(float(sim))\n",
    "\n",
    "            # popularity\n",
    "            vec.append(float(nb[\"popularity\"]))\n",
    "\n",
    "            # audio features\n",
    "            af = nb[\"audio_features\"]\n",
    "            for col in audio_feature_cols:\n",
    "                val = af.get(col, np.nan)\n",
    "                if isinstance(val, (float, int, np.floating)):\n",
    "                    vec.append(float(val))\n",
    "                else:\n",
    "                    vec.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            vec.extend([np.nan] * NEIGHBOR_BLOCK)\n",
    "\n",
    "    return np.asarray(vec, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_neighbors(df, query_embedding, k=5):\n",
    "    raw_neighbors = retrieve_similar_songs(query_embedding, k=k)\n",
    "    neighbors = []\n",
    "\n",
    "    for n in raw_neighbors:\n",
    "        idx = n[\"index\"]\n",
    "        row = df.iloc[idx]\n",
    "\n",
    "        audio_features = {}\n",
    "\n",
    "        for col in audio_feature_cols:\n",
    "            val = row[col]\n",
    "\n",
    "            # keep if scalar\n",
    "            if np.isscalar(val):\n",
    "                audio_features[col] = float(val)\n",
    "            \n",
    "            # flatten if array\n",
    "            elif isinstance(val, np.ndarray):\n",
    "                val = val.flatten()\n",
    "                for j, v in enumerate(val):\n",
    "                    audio_features[f\"{col}_{j}\"] = float(v)\n",
    "            \n",
    "            # flatten if list\n",
    "            elif isinstance(val, list):\n",
    "                for j, v in enumerate(val):\n",
    "                    audio_features[f\"{col}_{j}\"] = float(v)\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    audio_features[col] = float(val)\n",
    "                except Exception:\n",
    "                    audio_features[col] = None\n",
    "\n",
    "        neighbor_data = {\n",
    "            \"index\": idx,\n",
    "            \"song_id\": row[\"song_id\"],\n",
    "            \"title\": row[\"title\"],\n",
    "            \"artist\": row[\"artist\"],\n",
    "            \"similarity\": n.get(\"similarity\", None),\n",
    "            \"popularity\": float(row[\"popularity\"]),\n",
    "            \"lyrics_snippet\": row[\"lyrics\"][:400].replace(\"\\n\", \" \") + \"...\",\n",
    "            \"audio_features\": audio_features\n",
    "        }\n",
    "        \n",
    "        neighbors.append(neighbor_data)\n",
    "\n",
    "    return neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below evaluates our FAISS method for similary search. it rnadomly samples 'num_samples' songs to measure:\n",
    "\n",
    "- average recall@5: whether retrieved neighbors share the same genre as the target song\n",
    "\n",
    "- average absolute difference in popularity between a song and its retrieved neighbors\n",
    "\n",
    "- average FAISS L2 distance between each song and its nearest neighbor\n",
    "\n",
    "- average L2 distance between a lyric embedding and a random unrelated song\n",
    "\n",
    "- and the ratio between distance from a random song over top1 distance\n",
    "\t​\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'genre_recall_mean': 0.07200000000000001, 'pop_distance_mean': 21.936, 'top1_dist_mean': 0.6875483393661367, 'random_dist_mean': 1.1651294269164403, 'top1_vs_random_ratio': 1.6946145604694418}\n"
     ]
    }
   ],
   "source": [
    "### --- SIMILARITY SEARCH EVALUATION METRICS ---\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_similarity_search(df, k=5, num_samples=300):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval quality using:\n",
    "      - Genre recall@k\n",
    "      - Popularity distance\n",
    "      - FAISS distance diagnostics\n",
    "    Self-matches are removed so top1 distance is meaningful.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "    sample_indices = rng.choice(len(df), size=num_samples, replace=False)\n",
    "\n",
    "    recall_scores = []\n",
    "    pop_distances = []\n",
    "    top1_distances = []\n",
    "    random_distances = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        row = df.iloc[idx]\n",
    "        emb = np.asarray(row[\"lyrics_embedding\"], dtype=\"float32\")\n",
    "        genre = row[\"track_genre\"]\n",
    "        pop = row[\"popularity\"]\n",
    "\n",
    "        # Retrieve neighbors while excluding the track itself\n",
    "        raw = retrieve_similar_songs(emb, k=k+1)   # get extra\n",
    "        neighbors = []\n",
    "\n",
    "        for nb in raw:\n",
    "            if nb[\"index\"] != idx:   # remove self-match\n",
    "                neighbors.append(nb)\n",
    "            if len(neighbors) == k:\n",
    "                break\n",
    "\n",
    "        # Safety check in case retrieval gives fewer than k neighbors\n",
    "        if len(neighbors) == 0:\n",
    "            continue\n",
    "\n",
    "        # ---------- 1. Genre Recall ----------\n",
    "        same_genre = [\n",
    "            1 if df.iloc[nb[\"index\"]][\"track_genre\"] == genre else 0\n",
    "            for nb in neighbors\n",
    "        ]\n",
    "        recall_scores.append(sum(same_genre) / k)\n",
    "\n",
    "        # ---------- 2. Popularity Distance ----------\n",
    "        pop_diff = np.mean([abs(df.iloc[nb[\"index\"]][\"popularity\"] - pop)\n",
    "                            for nb in neighbors])\n",
    "        pop_distances.append(pop_diff)\n",
    "\n",
    "        # ---------- 3. Top-1 FAISS Distance ----------\n",
    "        top1_distances.append(neighbors[0][\"similarity\"])\n",
    "\n",
    "        # ---------- 4. Random Negative Distance ----------\n",
    "        rand_idx = rng.choice(len(df))\n",
    "        rand_emb = np.asarray(df.iloc[rand_idx][\"lyrics_embedding\"], dtype=\"float32\")\n",
    "        d_rand = float(np.linalg.norm(emb - rand_emb))\n",
    "        random_distances.append(d_rand)\n",
    "\n",
    "    return {\n",
    "        \"genre_recall_mean\": np.mean(recall_scores),\n",
    "        \"pop_distance_mean\": np.mean(pop_distances),\n",
    "        \"top1_dist_mean\": np.mean(top1_distances),\n",
    "        \"random_dist_mean\": np.mean(random_distances),\n",
    "        \"top1_vs_random_ratio\": np.mean(random_distances) / np.mean(top1_distances),\n",
    "    }\n",
    "\n",
    "\n",
    "# run it\n",
    "sim_eval = evaluate_similarity_search(df, k=5)\n",
    "print(sim_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity-search evaluation reveals that our lyric-based FAISS retrieval is primarily capturing semantic textual similarity rather than musical similarity. The Genre Recall@5 score of 0.072 indicates that only  about 7% of retrieved neighbors share the same genre as the target song, which means that the sentence-transformer embeddings emphasize lyrical meaning rather than audio driven genre distinctions. The average popularity distance of 21.9 unfortunately suggests that lyrical similarity does not strongly correlate with popularity, reinforcing that popularity is influenced by many non-lyrical factors. Importantly, the FAISS distance metrics show meaningful retrieval behavior: the average nearest-neighbor distance (0.688) is substantially smaller than the average random-pair distance (1.165), producing a top-1 vs random ratio of 1.69, confirming that the embedding space is structured enough for reliable neighbor retrieval. Overall, these results validate that the retrieval component of our RAG pipeline finds semantically similar lyric neighbors rather than genre- or popularity-matched songs, which aligns with our goal of providing lyrically grounded contextual evidence for downstream popularity explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_neighbor_similarity': 0.7681275606155396, 'mean_random_similarity': 0.315833181142807, 'semantic_coherence_score': 2.432067394256592}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_semantic_coherence(df, k=3, num_samples=200):\n",
    "    \"\"\"\n",
    "    Measures whether FAISS neighbors are more semantically similar \n",
    "    to the query lyric than random songs.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    sample_indices = rng.choice(len(df), size=num_samples, replace=False)\n",
    "\n",
    "    neighbor_sims = []\n",
    "    random_sims = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        row = df.iloc[idx]\n",
    "        query_emb = np.asarray(row[\"lyrics_embedding\"], dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "        # Retrieve neighbors\n",
    "        neighbors = get_top_k_neighbors(df, query_emb.flatten(), k=k)\n",
    "        \n",
    "        # Compute similarity to neighbors\n",
    "        for nb in neighbors:\n",
    "            emb_nb = np.asarray(df.iloc[nb[\"index\"]][\"lyrics_embedding\"], dtype=\"float32\").reshape(1, -1)\n",
    "            sim = cosine_similarity(query_emb, emb_nb)[0][0]\n",
    "            neighbor_sims.append(sim)\n",
    "\n",
    "        # Random comparison\n",
    "        rand_idx = rng.choice(len(df))\n",
    "        rand_emb = np.asarray(df.iloc[rand_idx][\"lyrics_embedding\"], dtype=\"float32\").reshape(1, -1)\n",
    "        random_sims.append(cosine_similarity(query_emb, rand_emb)[0][0])\n",
    "\n",
    "    return {\n",
    "        \"mean_neighbor_similarity\": float(np.mean(neighbor_sims)),\n",
    "        \"mean_random_similarity\": float(np.mean(random_sims)),\n",
    "        \"semantic_coherence_score\": float(np.mean(neighbor_sims) / np.mean(random_sims))\n",
    "    }\n",
    "print(compute_semantic_coherence(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_neighbor_audio_distance': 1542.5098876953125, 'mean_random_audio_distance': 2454.44873046875, 'audio_alignment_score': 1.5912045240402222}\n"
     ]
    }
   ],
   "source": [
    "def compute_audio_alignment(df, audio_feature_cols, k=3, num_samples=200):\n",
    "    \"\"\"\n",
    "    Compares audio feature distances between query->neighbors and query->random tracks.\n",
    "    Lower distance to neighbors indicates better multimodal retrieval alignment.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    sample_indices = rng.choice(len(df), size=num_samples, replace=False)\n",
    "\n",
    "    neighbor_dists = []\n",
    "    random_dists = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        row = df.iloc[idx]\n",
    "        query_audio = np.asarray(row[audio_feature_cols], dtype=\"float32\")\n",
    "\n",
    "        # Retrieve neighbors\n",
    "        neighbors = get_top_k_neighbors(df, row[\"lyrics_embedding\"], k=k)\n",
    "\n",
    "        # Compute neighbor distances\n",
    "        for nb in neighbors:\n",
    "            nb_audio = np.asarray(df.iloc[nb[\"index\"]][audio_feature_cols], dtype=\"float32\")\n",
    "            dist = np.linalg.norm(query_audio - nb_audio)\n",
    "            neighbor_dists.append(dist)\n",
    "\n",
    "        # Random comparison\n",
    "        rand_idx = rng.choice(len(df))\n",
    "        rand_audio = np.asarray(df.iloc[rand_idx][audio_feature_cols], dtype=\"float32\")\n",
    "        random_dists.append(np.linalg.norm(query_audio - rand_audio))\n",
    "\n",
    "    return {\n",
    "        \"mean_neighbor_audio_distance\": float(np.mean(neighbor_dists)),\n",
    "        \"mean_random_audio_distance\": float(np.mean(random_dists)),\n",
    "        \"audio_alignment_score\": float(np.mean(random_dists) / np.mean(neighbor_dists))\n",
    "    }\n",
    "print(compute_audio_alignment(df, audio_feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic coherence function compares the cosine similarity between a query lyric and its retrieved neighbors to the similarity between the same query and randomly sampled lyrics. This measures whether FAISS retrieval is capturing real semantic structure rather than random associations. 2.43 shows that neighbors are more than twice as semantically similar as random songs, indicating that lyric-based retrieval is working extremely well for our RAG pipeline. The audio alignment function computes the distance between a query’s audio features and those of its retrieved neighbors, then compares this distance to random audio pairs. This tests whether lyrics-based retrieval unintentionally preserves meaningful audio similarity—a desirable property for multimodal prediction. 1.59 shows that neighbors are substantially more audio-similar than random songs, suggesting that the retrieval stage benefits the multimodal nature of our popularity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    target_lyric = row[\"lyrics\"]\n",
    "    target_embedding = np.asarray(row[\"lyrics_embedding\"], dtype=\"float32\")\n",
    "\n",
    "    neighbors = get_top_k_neighbors(df, target_embedding, k=3)\n",
    "\n",
    "    x_vec = construct_feature_vector(target_embedding, neighbors, audio_feature_cols, k=3)\n",
    "\n",
    "    X.append(x_vec)\n",
    "    y.append(row[\"popularity\"])\n",
    "\n",
    "X = np.vstack(X)\n",
    "y = np.array(y, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 584457\n",
      "[LightGBM] [Info] Number of data points in the train set: 20740, number of used features: 2299\n",
      "[LightGBM] [Info] Start training from score 38.835680\n",
      "LightGBM model trained!\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X, label=y)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": [\"rmse\", \"mae\"],\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 63,\n",
    "    \"max_depth\": -1,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "}\n",
    "\n",
    "model_lgb = lgb.train(params, train_data, num_boost_round=500)\n",
    "\n",
    "print(\"LightGBM model trained!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model_lgb)\n",
    "shap_values = explainer.shap_values(x_vec.reshape(1, -1))\n",
    "\n",
    "def summarize_shap_for_sample(\n",
    "    shap_values: np.ndarray,\n",
    "    feature_names: List[str] = None,\n",
    "    top_n: int = 15\n",
    ") -> List[Dict[str, Any]]:\n",
    "\n",
    "    if shap_values.ndim == 2:\n",
    "        shap_vals = shap_values[0]\n",
    "    else:\n",
    "        shap_vals = shap_values\n",
    "\n",
    "    abs_vals = np.abs(shap_vals)\n",
    "    top_idx = np.argsort(abs_vals)[::-1][:top_n]\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(len(shap_vals))]\n",
    "\n",
    "    summary = []\n",
    "    for idx in top_idx:\n",
    "        summary.append({\n",
    "            \"feature\": feature_names[idx],\n",
    "            \"shap_value\": float(shap_vals[idx])\n",
    "        })\n",
    "    return summary\n",
    "\n",
    "\n",
    "def group_shap_fully(\n",
    "    shap_vals: np.ndarray,\n",
    "    EMB_DIM: int,\n",
    "    audio_feature_cols: List[str],\n",
    "    k_neighbors: int = 3\n",
    "):\n",
    "\n",
    "    if shap_vals.ndim == 2:\n",
    "        shap_vals = shap_vals[0]\n",
    "\n",
    "    idx = 0\n",
    "    groups = {}\n",
    "\n",
    "    AUDIO_DIM = len(audio_feature_cols)\n",
    "\n",
    "    target_emb_shap = shap_vals[idx : idx + EMB_DIM]\n",
    "    groups[\"target_embedding\"] = float(np.sum(target_emb_shap))\n",
    "    idx += EMB_DIM\n",
    "\n",
    "    groups[\"neighbors\"] = []\n",
    "\n",
    "    for nb in range(k_neighbors):\n",
    "\n",
    "        # group by embeddings\n",
    "        emb_block = shap_vals[idx : idx + EMB_DIM]\n",
    "        emb_sum = float(np.sum(emb_block))\n",
    "        idx += EMB_DIM\n",
    "\n",
    "        # group by similarity\n",
    "        sim_shap = float(shap_vals[idx])\n",
    "        idx += 1\n",
    "\n",
    "        # group by popularity\n",
    "        pop_shap = float(shap_vals[idx])\n",
    "        idx += 1\n",
    "\n",
    "        # dont group by audio features\n",
    "        audio_block = shap_vals[idx : idx + AUDIO_DIM]\n",
    "        idx += AUDIO_DIM\n",
    "        \n",
    "        audio_dict = {\n",
    "            feature_name: float(audio_block[j])\n",
    "            for j, feature_name in enumerate(audio_feature_cols)\n",
    "        }\n",
    "\n",
    "        groups[\"neighbors\"].append({\n",
    "            \"embedding\": emb_sum,\n",
    "            \"similarity\": sim_shap,\n",
    "            \"popularity\": pop_shap,\n",
    "            \"audio_features\": audio_dict\n",
    "        })\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue working on promping\n",
    "def build_rag_prompt_for_lyric_popularity(\n",
    "    user_lyric: str,\n",
    "    neighbors: List[Dict[str, Any]],\n",
    "    predicted_popularity: float,\n",
    "    shap_summary: List[Dict[str, Any]]\n",
    "):\n",
    "\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"You are an expert in music analytics, audio features, and lyric interpretation.\")\n",
    "    lines.append(\"Your task is to EXPLAIN a predicted popularity score for a NEW lyric.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"CRITICAL INSTRUCTIONS:\")\n",
    "    lines.append(\" - DO NOT provide disclaimers about limitations of predicting popularity from lyrics.\")\n",
    "    lines.append(\" - Ground every part of your explanation in the retrieved similar songs.\")\n",
    "    lines.append(\" - Quote specific phrases from the neighbor lyrics when helpful.\")\n",
    "    lines.append(\" - Explain audio features in simple, everyday terms.\")\n",
    "    lines.append(\" - Use the SHAP feature-attribution summary as evidence for WHY the model made its prediction.\")\n",
    "    lines.append(\" - Structure your explanation into multiple paragraphs:\")\n",
    "    lines.append(\"      Paragraph 1: Lyric similarity analysis using retrieved neighbors.\")\n",
    "    lines.append(\"      Paragraph 2: Audio feature comparisons (brightness, timbre, tempo, etc.).\")\n",
    "    lines.append(\"      Paragraph 3: Interpretation of SHAP results for this specific lyric.\")\n",
    "    lines.append(\"      Paragraph 4: Final justification tying all evidence together.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Return ONLY valid JSON with this format:\")\n",
    "    lines.append(\"{\")\n",
    "    lines.append('  \"predicted_popularity\": <number>,')\n",
    "    lines.append('  \"explanation\": \"<multi-paragraph explanation grounded in evidence>\"')\n",
    "    lines.append(\"}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"IMPORTANT:\")\n",
    "    lines.append(\"Return ONLY raw JSON.\")\n",
    "    lines.append(\"Do NOT include any code fences such as ``` json\")\n",
    "    lines.append(\"Do NOT include any explanation text outside the JSON.\")\n",
    "    lines.append(\"Do NOT add commentary before or after the JSON.\")\n",
    "    lines.append(\"Return JSON ONLY.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"------------------------------------------------------------\")\n",
    "    lines.append(\"NEW LYRIC:\")\n",
    "    lines.append(user_lyric.strip())\n",
    "    lines.append(\"------------------------------------------------------------\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Predicted Popularity Score: {predicted_popularity:.2f}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"------------------------------------------------------------\")\n",
    "    lines.append(\"SHAP FEATURE-ATTRIBUTION SUMMARY (TOP CONTRIBUTORS):\")\n",
    "    lines.append(\"These features influenced the model's prediction and should be used in the explanation:\")\n",
    "    lines.append(\"SHAP FEATURE ATTRIBUTION SUMMARY (GROUPED):\")\n",
    "    lines.append(f\"  Target embedding contribution: {shap_summary['target_embedding']:+.3f}\")\n",
    "\n",
    "    lines.append(\"\\nNeighbor Contributions:\")\n",
    "    for i, nb in enumerate(shap_summary[\"neighbors\"], start=1):\n",
    "        lines.append(f\"  Neighbor {i}:\")\n",
    "        lines.append(f\"    embedding: {nb['embedding']:+.3f}\")\n",
    "        lines.append(f\"    similarity: {nb['similarity']:+.3f}\")\n",
    "        lines.append(f\"    popularity: {nb['popularity']:+.3f}\")\n",
    "        lines.append(\"    audio_features:\")\n",
    "        for feat_name, val in nb[\"audio_features\"].items():\n",
    "            lines.append(f\"      {feat_name}: {val:+.3f}\")\n",
    "\n",
    "    lines.append(\"------------------------------------------------------------\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"SIMILAR SONGS RETRIEVED FROM THE DATASET:\")\n",
    "    lines.append(\"Use these songs as evidence for lyrical themes, audio patterns, and overall justification.\")\n",
    "\n",
    "    for i, nb in enumerate(neighbors, start=1):\n",
    "        lines.append(f\"\\nNeighbor #{i}:\")\n",
    "        lines.append(f\"  song_id: {nb['song_id']}\")\n",
    "        lines.append(f\"  title: {nb['title']}\")\n",
    "        lines.append(f\"  artist: {nb['artist']}\")\n",
    "        if nb.get(\"similarity\") is not None:\n",
    "            lines.append(f\"  similarity_score: {nb['similarity']:.4f}  (lower = more similar lyrics)\")\n",
    "        lines.append(f\"  popularity: {nb['popularity']:.2f}\")\n",
    "        lines.append(f\"  lyrics_snippet: {nb['lyrics_snippet']}\")\n",
    "        lines.append(\"  audio_features:\")\n",
    "        for feat_name, feat_val in nb[\"audio_features\"].items():\n",
    "            # print with 4-decimal formatting if numeric\n",
    "            if isinstance(feat_val, (int, float)):\n",
    "                lines.append(f\"    {feat_name}: {feat_val:.4f}\")\n",
    "            else:\n",
    "                lines.append(f\"    {feat_name}: {feat_val}\")\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"------------------------------------------------------------\")\n",
    "    lines.append(\n",
    "        \"Using ONLY the information above — the new lyric, retrieved neighbors, \"\n",
    "        \"the predicted popularity score, and the SHAP feature-attribution summary — \"\n",
    "        \"produce a multi-paragraph explanation grounded in the dataset evidence. \"\n",
    "        \"Do not speculate beyond what is shown. Do not include disclaimers. \"\n",
    "        \"Focus on clear, real-world intuition about audio features, lyrical patterns, \"\n",
    "        \"genre cues, and model attribution.\"\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "def call_llm_for_popularity_and_explanation(prompt: str) -> dict:\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=prompt,\n",
    "        temperature=0.2,\n",
    "        max_output_tokens=900\n",
    "    )\n",
    "\n",
    "    raw_text = response.output[0].content[0].text.strip()\n",
    "\n",
    "    # Remove any ```json ...``` or ```\n",
    "    raw_text = raw_text.replace(\"```json\", \"\")\n",
    "    raw_text = raw_text.replace(\"```\", \"\")\n",
    "    raw_text = raw_text.strip()\n",
    "\n",
    "    # first try direct json parse\n",
    "    try:\n",
    "        return json.loads(raw_text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # else, find outside json block using regex\n",
    "    json_matches = re.findall(r\"\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}\", raw_text, flags=re.DOTALL)\n",
    "\n",
    "    if json_matches:\n",
    "        for match in json_matches:\n",
    "            try:\n",
    "                return json.loads(match)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # try to repair json with trailing commas\n",
    "    repaired = re.sub(r\",\\s*([}\\]])\", r\"\\1\", raw_text)\n",
    "\n",
    "    try:\n",
    "        return json.loads(repaired)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # else, say failed\n",
    "    print(\"Could not parse JSON from LLM output. Returning raw text.\")\n",
    "    return {\n",
    "        \"predicted_popularity\": None,\n",
    "        \"explanation\": raw_text\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_lyric_popularity_system(\n",
    "    df: pd.DataFrame,\n",
    "    user_lyric: str,\n",
    "    k_neighbors: int = 3,\n",
    "    top_shap_features: int = 15,\n",
    "    feature_names: List[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      1) Embed new lyric\n",
    "      2) Retrieve top-k similar songs\n",
    "      3) Build multimodal feature vector\n",
    "      4) Predict popularity with LightGBM\n",
    "      5) Get TreeSHAP local explanation\n",
    "      6) Build RAG prompt and call LLM for explanation\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) embed target lyric\n",
    "    query_embedding = embed_lyrics(user_lyric)\n",
    "\n",
    "    # 2) retrieve neighbors using FAISS + your helper\n",
    "    neighbors = get_top_k_neighbors(df, query_embedding, k=k_neighbors)\n",
    "\n",
    "    # 3) construct feature vector for prediction\n",
    "    x_vec = construct_feature_vector(\n",
    "        target_embedding=query_embedding,\n",
    "        neighbors=neighbors,\n",
    "        audio_feature_cols=audio_feature_cols,\n",
    "        k=k_neighbors\n",
    "    )\n",
    "\n",
    "    # 4) predict popularity with LightGBM\n",
    "    pred_pop = float(model_lgb.predict(x_vec.reshape(1, -1))[0])\n",
    "\n",
    "    # 5) compute SHAP values for this sample\n",
    "    shap_vals = explainer.shap_values(x_vec.reshape(1, -1))\n",
    "\n",
    "    EMB_DIM = len(query_embedding)\n",
    "    shap_grouped = group_shap_fully(\n",
    "        shap_vals,\n",
    "        EMB_DIM=EMB_DIM,\n",
    "        audio_feature_cols=audio_feature_cols,\n",
    "        k_neighbors=k_neighbors\n",
    "    )\n",
    "\n",
    "\n",
    "    # 6) build prompt for explanation (we reconstructed this earlier)\n",
    "    prompt = build_rag_prompt_for_lyric_popularity(\n",
    "        user_lyric=user_lyric,\n",
    "        neighbors=neighbors,\n",
    "        predicted_popularity=pred_pop,\n",
    "        shap_summary=shap_grouped\n",
    "    )\n",
    "\n",
    "    # 7) call LLM for explanation ONLY\n",
    "    llm_output = call_llm_for_popularity_and_explanation(prompt)\n",
    "\n",
    "    # we trust our model's pred_pop more than anything the LLM might echo back\n",
    "    explanation = llm_output.get(\"explanation\", \"\")\n",
    "\n",
    "    return {\n",
    "        \"predicted_popularity\": pred_pop,\n",
    "        \"explanation\": explanation,\n",
    "        \"neighbors_used\": neighbors,\n",
    "        \"prompt_sent\": prompt,\n",
    "        \"raw_llm_output\": llm_output,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shap_retrieval_correlation': 0.03869600948713251}\n"
     ]
    }
   ],
   "source": [
    "def compute_shap_retrieval_consistency(df, explainer, k=3, num_samples=100):\n",
    "    \"\"\"\n",
    "    Correlates neighbor similarity with SHAP magnitude to see if more similar neighbors\n",
    "    influence predictions more strongly.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(1)\n",
    "    idxs = rng.choice(len(df), size=num_samples, replace=False)\n",
    "\n",
    "    all_sims = []\n",
    "    all_shaps = []\n",
    "\n",
    "    for idx in idxs:\n",
    "        row = df.iloc[idx]\n",
    "        query_emb = np.asarray(row[\"lyrics_embedding\"], dtype=\"float32\")\n",
    "\n",
    "        # Retrieve neighbors\n",
    "        neighbors = get_top_k_neighbors(df, query_emb, k=k)\n",
    "\n",
    "        # Construct feature vector\n",
    "        x_vec = construct_feature_vector(query_emb, neighbors, audio_feature_cols, k)\n",
    "\n",
    "        # SHAP for sample\n",
    "        shap_vals = explainer.shap_values(x_vec.reshape(1, -1))[0]\n",
    "\n",
    "        EMB_DIM = len(query_emb)\n",
    "        idx_ptr = EMB_DIM  # skip target embedding block\n",
    "\n",
    "        # For each neighbor, extract SHAP and match to similarity\n",
    "        for nb in neighbors:\n",
    "            sim = nb[\"similarity\"]\n",
    "\n",
    "            # SHAP blocks: [emb], [similarity], [pop], [audio]\n",
    "            emb_shap_sum = np.sum(shap_vals[idx_ptr : idx_ptr + EMB_DIM])\n",
    "            idx_ptr += EMB_DIM\n",
    "\n",
    "            sim_shap = shap_vals[idx_ptr]\n",
    "            idx_ptr += 1\n",
    "            idx_ptr += 1 + len(audio_feature_cols)  # skip pop + audio block\n",
    "\n",
    "            # Use absolute SHAP for stability\n",
    "            all_sims.append(sim)\n",
    "            all_shaps.append(abs(sim_shap))\n",
    "\n",
    "    # Pearson correlation\n",
    "    correlation = np.corrcoef(all_sims, all_shaps)[0, 1]\n",
    "\n",
    "    return {\n",
    "        \"shap_retrieval_correlation\": float(correlation)\n",
    "    }\n",
    "print(compute_shap_retrieval_consistency(df, explainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function correlates neighbor similarity (how close retrieved songs are to the query) with the magnitude of SHAP contributions those neighbors exert on the prediction. A high correlation would indicate that the prediction model relies on the semantic closeness of retrieved neighbors. The correlation of 0.039, however, shows that similarity plays little direct role in the model’s decision process, suggesting the predictor may be drawing more heavily on target features or audio attributes rather than neighbor similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted popularity: 68.35083016646738\n",
      "\n",
      "Explanation:\n",
      " The new lyric 'im not cute anymore' shares thematic elements with the retrieved neighbors, particularly in expressing a sense\n",
      "of self-awareness and emotional complexity. For instance, Neighbor 1, 'LIKEY' by TWICE, includes phrases like 'Me likey,\n",
      "likey, likey,' which convey a playful yet introspective tone about self-image and perception. Similarly, Neighbor 2, 'Mateo'\n",
      "by Tove Lo, explores themes of self-doubt and longing, as seen in 'I act so cool, but that's not me.' These lyrical\n",
      "similarities suggest a relatable and introspective quality that resonates with listeners, contributing to the predicted\n",
      "popularity score.  In terms of audio features, the new lyric's predicted popularity is influenced by its alignment with the\n",
      "musical characteristics of its neighbors. Neighbor 1, 'LIKEY,' features a bright and energetic sound with a tempo of 130.8140\n",
      "BPM and a high spectral centroid, indicating a lively and engaging track. Neighbor 2, 'Mateo,' has a slightly faster tempo of\n",
      "148.0263 BPM and a rich spectral contrast, suggesting a dynamic and emotionally charged sound. These audio features\n",
      "contribute to the overall appeal and accessibility of the songs, enhancing their popularity.  The SHAP feature-attribution\n",
      "summary highlights the significant contributions from Neighbor 1's popularity and Neighbor 2's embedding similarity. Neighbor\n",
      "1's high popularity score of 68.00 strongly influences the prediction, indicating that songs with similar themes and audio\n",
      "features have historically performed well. Additionally, the embedding similarity from Neighbor 2 suggests that the new lyric\n",
      "captures a similar emotional and thematic essence, further supporting its potential popularity.  Overall, the predicted\n",
      "popularity score of 68.35 is justified by the combination of lyrical themes, audio features, and historical performance of\n",
      "similar songs. The introspective and relatable nature of the lyric, coupled with engaging audio characteristics, aligns well\n",
      "with successful tracks in the dataset, leading to a favorable prediction.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "test_lyric = \"im not cute anymore\"\n",
    "result = rag_lyric_popularity_system(df, test_lyric, k_neighbors=3)\n",
    "\n",
    "print(\"Predicted popularity:\", result[\"predicted_popularity\"])\n",
    "print(\"\\nExplanation:\\n\", textwrap.fill(result[\"explanation\"], width=125))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_json_rate': 1.0, 'repaired_json_rate': 0.0, 'failed_rate': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def evaluate_llm_json_validity(df, num_samples=100):\n",
    "    \"\"\"\n",
    "    Run pipeline randomly to measure JSON validity rate.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    idxs = rng.choice(len(df), size=num_samples, replace=False)\n",
    "    \n",
    "    valid = 0\n",
    "    repaired = 0\n",
    "    failed = 0\n",
    "\n",
    "    for idx in idxs:\n",
    "        lyric = df.iloc[idx][\"lyrics\"]\n",
    "        result = rag_lyric_popularity_system(df, lyric, k_neighbors=3)\n",
    "\n",
    "        raw = result[\"raw_llm_output\"]\n",
    "\n",
    "        if raw[\"predicted_popularity\"] is not None:\n",
    "            valid += 1\n",
    "        elif raw[\"explanation\"].startswith(\"{\"):\n",
    "            repaired += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "    return {\n",
    "        \"valid_json_rate\": valid / num_samples,\n",
    "        \"repaired_json_rate\": repaired / num_samples,\n",
    "        \"failed_rate\": failed / num_samples\n",
    "    }\n",
    "\n",
    "json_eval = evaluate_llm_json_validity(df, num_samples=30)\n",
    "print(json_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evidence_grounding_rate': 1.0, 'hallucination_rate': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_evidence_grounding(df, num_samples=50):\n",
    "    rng = np.random.default_rng(1)\n",
    "    idxs = rng.choice(len(df), size=num_samples, replace=False)\n",
    "\n",
    "    grounded = 0\n",
    "    hallucinated = 0\n",
    "\n",
    "    for idx in idxs:\n",
    "        row = df.iloc[idx]\n",
    "        lyric = row[\"lyrics\"]\n",
    "\n",
    "        result = rag_lyric_popularity_system(df, lyric, 3)\n",
    "        text = result[\"explanation\"].lower()\n",
    "\n",
    "        neighbor_names = [\n",
    "            nb[\"title\"].lower() for nb in result[\"neighbors_used\"]\n",
    "        ] + [\n",
    "            nb[\"artist\"].lower() for nb in result[\"neighbors_used\"]\n",
    "        ]\n",
    "\n",
    "        # grounded?\n",
    "        if any(name in text for name in neighbor_names):\n",
    "            grounded += 1\n",
    "\n",
    "        # hallucination: detect mention of artist not in neighbors\n",
    "        all_artists = set(df[\"artist\"].str.lower().unique())\n",
    "        neighbor_set = set(name.lower() for name in neighbor_names)\n",
    "\n",
    "        # words that look like artists but aren't neighbors\n",
    "        mentioned_artists = [a for a in all_artists if a in text]\n",
    "        for a in mentioned_artists:\n",
    "            if a not in neighbor_set:\n",
    "                hallucinated += 1\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"evidence_grounding_rate\": grounded / num_samples,\n",
    "        \"hallucination_rate\": hallucinated / num_samples\n",
    "    }\n",
    "\n",
    "ground_eval = evaluate_evidence_grounding(df)\n",
    "print(ground_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running test lyric 1 ---\n",
      "\n",
      "--- Running test lyric 2 ---\n",
      "\n",
      "--- Running test lyric 3 ---\n",
      "\n",
      "--- Running test lyric 4 ---\n",
      "\n",
      "--- Running test lyric 5 ---\n",
      "\n",
      "--- Running test lyric 6 ---\n",
      "{'grounding_rate': 1.0, 'results_per_lyric': [{'lyric': \"I'm not the same as I once was\", 'grounded': True, 'mentioned_titles': ['open your eyes', 'glue', 'same girl'], 'mentioned_artists': ['s.p.y', 'bicep', 'randy newman'], 'raw_explanation': \"The new lyric 'I'm not the same as I once was' shares thematic elements with Neighbor #1, 'Open Your Eyes' by S.P.Y, which includes phrases like 'but I'll never be the same.' This similarity in lyrical content suggests a theme of change and transformation. Neighbor #3, 'Same Girl' by Randy Newman, also explores themes of identity and change, with lines such as 'Still the same girl you always were.' These thematic parallels contribute to the model's understanding of the new lyric's potential appeal, though the popularity of these neighbors varies significantly, with Neighbor #1 having a lower popularity score.\\n\\nIn terms of audio features, Neighbor #1 has a bright and energetic sound, indicated by a high spectral centroid and zero crossing rate, which contribute to a lively and dynamic feel. Neighbor #2, 'Glue' by Bicep, has a lower tempo and a more subdued spectral profile, suggesting a more laid-back and ambient sound. Neighbor #3, 'Same Girl,' features a higher tempo and a more varied spectral contrast, indicating a more complex and layered sound. These audio characteristics provide context for the new lyric's potential sound profile, which may lean towards a more energetic or dynamic production style.\\n\\nThe SHAP feature-attribution summary highlights that the target embedding contribution is slightly negative, suggesting that the model's internal representation of the new lyric is not strongly aligned with highly popular songs. Neighbor #1's low popularity score significantly impacts the prediction, with a large negative contribution. However, Neighbor #2's higher popularity provides a slight positive influence, balancing the overall prediction. The audio features, such as spectral contrast and MFCCs, show mixed contributions, with some features like spectral contrast 3 and 4 having negative impacts, indicating that certain audio characteristics may not align with more popular tracks.\\n\\nOverall, the predicted popularity score of 16.07 is justified by the combination of lyrical themes and audio features observed in the neighbors. The thematic similarity to songs about change and identity, combined with the varied audio profiles, suggests a niche appeal. The negative contributions from certain audio features and the low popularity of Neighbor #1 further explain the moderate predicted popularity score.\"}, {'lyric': 'I am the eggman, they are the eggmen', 'grounded': True, 'mentioned_titles': ['der eiermann', 'udi udi jaye', 'jajaja'], 'mentioned_artists': ['klaus & klaus', 'sukhwinder singh', 'plastilina mosh'], 'raw_explanation': \"The new lyric 'I am the eggman, they are the eggmen' shares thematic elements with Neighbor 1, 'Der Eiermann' by Klaus & Klaus, which also features playful repetition and whimsical references to eggmen. This similarity in lyrical content, with phrases like 'Hier kommt der Eiermann,' suggests a humorous and light-hearted tone. Neighbor 2, 'Udi Udi Jaye' by Sukhwinder Singh, and Neighbor 3, 'JAJAJA' by Plastilina Mosh, both exhibit repetitive and catchy phrases, contributing to their memorability, which is a common trait in popular songs.\\n\\nIn terms of audio features, Neighbor 1 has a moderate tempo of 102 BPM, which is similar to the upbeat tempos found in Neighbor 2 and Neighbor 3, both around 122-125 BPM. This suggests that the new lyric might be set to a lively and engaging rhythm. The spectral features, such as spectral centroid and bandwidth, indicate a bright and energetic sound, particularly in Neighbor 1, which aligns with the playful nature of the lyrics. The zero-crossing rate and spectral rolloff values further support a dynamic and vibrant audio profile.\\n\\nThe SHAP feature-attribution summary highlights that Neighbor 2's popularity had a positive influence on the prediction, contributing +0.214 to the score. This suggests that the model associates the new lyric with elements that are present in more popular songs. However, Neighbor 1's lower popularity score negatively impacted the prediction by -7.718, indicating that while the lyrical similarity is strong, the overall popularity of similar songs is mixed. The tonal features, such as tonnetz and chroma, show minor contributions, suggesting that the harmonic content is not a major factor in this prediction.\\n\\nOverall, the predicted popularity score of 31.36 is justified by the combination of lyrical similarity to playful and repetitive songs, as well as audio features that suggest an upbeat and engaging sound. The SHAP analysis indicates that while the lyrical style is consistent with some popular songs, the overall influence of less popular neighbors tempers the prediction, resulting in a moderate popularity score.\"}, {'lyric': 'One two buckle my shoe', 'grounded': True, 'mentioned_titles': ['higher power', 'tungs', 'nos cadenas'], 'mentioned_artists': ['coldplay', 'the frights', 'sex prisoner'], 'raw_explanation': \"The new lyric 'One two buckle my shoe' shares thematic elements with the retrieved neighbors, particularly in its simplicity and rhythmic structure. Neighbor 1, 'Higher Power' by Coldplay, includes a playful mention of shoes with 'I think my shoe's untied,' which resonates with the nursery rhyme style of the new lyric. Neighbor 2, 'Tungs' by The Frights, also features a counting sequence in its intro, 'One, two, three, four,' which aligns with the counting nature of the new lyric. These similarities suggest a playful, rhythmic quality that is common in children's rhymes and certain pop songs.\\n\\nIn terms of audio features, the retrieved neighbors exhibit a range of characteristics. Neighbor 1 has a moderate tempo and a bright sound, indicated by its high spectral centroid and bandwidth. Neighbor 2 has a slightly faster tempo and a similar brightness, while Neighbor 3, 'Nos Cadenas' by Sex Prisoner, features a much higher tempo and even brighter sound. The new lyric's predicted audio features likely align more closely with Neighbor 1 and 2, suggesting a moderate tempo and brightness that is typical of pop and rock genres.\\n\\nThe SHAP feature-attribution summary highlights that the target embedding contribution is significantly negative, suggesting that the model perceives the new lyric as less popular based on its embedding. Neighbor 1's high negative popularity score heavily influences the prediction, while Neighbor 2's positive popularity provides a slight counterbalance. The audio features from Neighbor 1, such as spectral contrast and MFCCs, contribute positively, indicating that the model finds some favorable audio characteristics in the new lyric.\\n\\nOverall, the predicted popularity score of -0.23 is primarily driven by the negative influence of Neighbor 1's low popularity and the target embedding's contribution. While the new lyric shares some playful and rhythmic qualities with popular songs, the overall embedding and influence from less popular neighbors result in a lower predicted popularity. The audio features suggest a moderate and bright sound, but the lyrical simplicity and thematic alignment with less popular songs weigh down the score.\"}, {'lyric': 'Going back to honolulu just to get that maui wowie', 'grounded': True, 'mentioned_titles': ['nini', 'hey wanhaka'], 'mentioned_artists': ['6ix9ine', 'king stingray'], 'raw_explanation': \"The new lyric 'Going back to honolulu just to get that maui wowie' shares thematic elements with Neighbor 1, 'NINI' by 6ix9ine, which includes phrases like 'Gyal dem come again' and 'Sexy lady, dame, dame.' Both lyrics evoke a sense of travel and exoticism, with a focus on specific locations and a playful tone. This similarity in lyrical content contributes to the model's prediction, as Neighbor 1 has a popularity score of 48.00, closely aligning with the predicted score.\\n\\nIn terms of audio features, the new lyric's predicted popularity is influenced by its similarity to the audio characteristics of the neighbors. Neighbor 1, for instance, has a bright and energetic sound, indicated by a high spectral centroid and tempo. The presence of these features suggests a lively and engaging musical backdrop, which is often associated with popular tracks. The spectral contrast and tonnetz values also contribute to a rich and dynamic sound, enhancing the overall appeal.\\n\\nThe SHAP feature-attribution summary highlights that Neighbor 2's embedding and similarity scores positively influence the prediction, despite its lower popularity. This suggests that the model values the lyrical and audio resemblance to Neighbor 2, 'Hey Wanhaka' by King Stingray, which emphasizes staying true to roots and a connection to place. The model's attribution indicates that these thematic elements resonate with the new lyric, contributing to its predicted popularity.\\n\\nOverall, the predicted popularity score of 48.10 is justified by the combination of lyrical similarity to popular themes found in Neighbor 1, the engaging audio features shared with the neighbors, and the positive influence of Neighbor 2's thematic elements. The model effectively integrates these factors, resulting in a score that reflects the potential appeal of the new lyric within the context of the dataset.\"}, {'lyric': 'last christmas i gave you my heart', 'grounded': True, 'mentioned_titles': ['last christmas', 'last christmas', 'last christmas'], 'mentioned_artists': ['jimmy eat world', 'wham!', 'the bosshoss'], 'raw_explanation': \"The new lyric 'last christmas i gave you my heart' closely resembles the lyrics of Neighbor 1, 'Last Christmas' by Jimmy Eat World, with a similarity score of 0.6764. This song features the same thematic elements of giving one's heart and the subsequent emotional fallout, as seen in phrases like 'Last Christmas, I gave you my heart.' However, despite the lyrical similarity, this neighbor has a low popularity score of 0.00, which negatively impacts the predicted popularity of the new lyric. Neighbor 2, 'Last Christmas' by Wham!, shares similar themes and has a higher popularity score of 74.00, but the similarity score is slightly higher at 0.6858, indicating less similarity compared to Neighbor 1. Neighbor 3, 'Last Christmas' by The BossHoss, also shares thematic elements but has a low popularity score of 0.00, similar to Neighbor 1.\\n\\nIn terms of audio features, Neighbor 1 has a moderate tempo of 125 BPM and a bright sound characterized by a high spectral centroid and spectral bandwidth. The zero-crossing rate, which indicates the noisiness of the signal, is relatively high, contributing to a crisp and energetic sound. Neighbor 2, by Wham!, has a slightly slower tempo of 108 BPM and a higher spectral rolloff, indicating a brighter sound. The zero-crossing rate is higher than Neighbor 1, suggesting a more pronounced brightness and energy. Neighbor 3 has a tempo of 110 BPM and similar spectral characteristics to Neighbor 1, with a slightly lower spectral centroid and bandwidth, contributing to a less bright sound.\\n\\nThe SHAP feature-attribution summary highlights that the target embedding contribution is negative, at -0.176, indicating that the model perceives the new lyric as less likely to be popular based on its embedding. Neighbor 1's embedding contribution is also negative, at -0.067, and its low popularity score of -38.066 significantly drags down the prediction. In contrast, Neighbor 2's embedding contribution is positive, at +0.080, and its high popularity score of +0.071 provides a positive influence, though not enough to outweigh the negative impacts from Neighbor 1. Neighbor 3's contributions are minor, with a slight negative embedding impact of -0.020 and a small positive popularity influence of +0.033.\\n\\nOverall, the predicted popularity score of 0.39 is primarily influenced by the strong lyrical similarity to Neighbor 1, which has a low popularity score. The audio features suggest a bright and energetic sound, similar to popular versions of 'Last Christmas,' but the negative embedding contributions and the low popularity of the most similar neighbor weigh heavily on the prediction. The positive influence from Neighbor 2's popularity is not sufficient to overcome these negative factors, resulting in a lower predicted popularity score for the new lyric.\"}, {'lyric': 'im not cute anymore', 'grounded': True, 'mentioned_titles': ['likey', 'mateo', 'wo bist du'], 'mentioned_artists': ['twice', 'tove lo', 'rammstein'], 'raw_explanation': \"The new lyric 'im not cute anymore' shares thematic elements with the retrieved songs, particularly in its exploration of self-perception and identity. Neighbor 1, 'LIKEY' by TWICE, discusses themes of self-image and the desire to be perceived as attractive, with phrases like '자꾸 드러내고 싶지' (I want to show off) and '내가 제일 예뻐 보이고파' (I want to look the prettiest). Similarly, Neighbor 2, 'Mateo' by Tove Lo, touches on feelings of inadequacy and comparison, as seen in 'Wish I could have some more time alone with you' and 'filled with pretty girls.' These lyrical similarities suggest a relatable and introspective theme that resonates with listeners, contributing to the predicted popularity score.\\n\\nIn terms of audio features, the songs have distinct characteristics that influence their appeal. Neighbor 1 has a bright and energetic sound, with a high spectral centroid and tempo, creating an upbeat and catchy feel. Neighbor 2, while slightly less energetic, maintains a lively tempo and a rich spectral contrast, which adds depth and texture to the music. Neighbor 3, 'Wo bist du' by Rammstein, features a darker, more intense sound with a lower spectral centroid and higher spectral bandwidth, contributing to its dramatic and emotional tone. These audio features suggest that the new lyric could be paired with a dynamic and engaging musical arrangement to enhance its appeal.\\n\\nThe SHAP feature-attribution summary highlights the significant contributions from Neighbor 1's popularity and Neighbor 2's embedding similarity. The high popularity of Neighbor 1, with a score of 68.00, strongly influences the prediction, indicating that songs with similar themes and audio characteristics have been successful. The embedding similarity from Neighbor 2 further supports this, as it suggests a close alignment in lyrical content and style, which the model associates with potential popularity.\\n\\nOverall, the predicted popularity score of 68.35 is justified by the combination of relatable lyrical themes, engaging audio features, and the influence of successful similar songs. The introspective nature of the new lyric, coupled with the potential for a dynamic musical arrangement, aligns well with the characteristics of popular songs in the dataset, supporting the model's prediction.\"}]}\n"
     ]
    }
   ],
   "source": [
    "test_lyrics = [\n",
    "    \"I'm not the same as I once was\",\n",
    "    \"I am the eggman, they are the eggmen\",\n",
    "    \"One two buckle my shoe\",\n",
    "    \"Going back to honolulu just to get that maui wowie\",\n",
    "    \"last christmas i gave you my heart\",\n",
    "    \"im not cute anymore\",\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_grounding_on_custom_lyrics(test_lyrics, df, k_neighbors=3):\n",
    "\n",
    "    results = []\n",
    "    grounded_count = 0\n",
    "\n",
    "    for i, lyric in enumerate(test_lyrics):\n",
    "        print(f\"\\n--- Running test lyric {i+1} ---\")\n",
    "\n",
    "        # Run your RAG pipeline\n",
    "        out = rag_lyric_popularity_system(df, lyric, k_neighbors=k_neighbors)\n",
    "\n",
    "        explanation = out[\"explanation\"].lower()\n",
    "        neighbors = out[\"neighbors_used\"]\n",
    "\n",
    "        # collect neighbor titles and artists\n",
    "        neighbor_titles = [nb[\"title\"].lower() for nb in neighbors]\n",
    "        neighbor_artists = [nb[\"artist\"].lower() for nb in neighbors]\n",
    "\n",
    "        # Check grounding (does the explanation mention at least one?)\n",
    "        grounded = (\n",
    "            any(title in explanation for title in neighbor_titles) or\n",
    "            any(artist in explanation for artist in neighbor_artists)\n",
    "        )\n",
    "\n",
    "        if grounded:\n",
    "            grounded_count += 1\n",
    "\n",
    "        results.append({\n",
    "            \"lyric\": lyric,\n",
    "            \"grounded\": grounded,\n",
    "            \"mentioned_titles\": [t for t in neighbor_titles if t in explanation],\n",
    "            \"mentioned_artists\": [a for a in neighbor_artists if a in explanation],\n",
    "            \"raw_explanation\": out[\"explanation\"]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"grounding_rate\": grounded_count / len(test_lyrics),\n",
    "        \"results_per_lyric\": results\n",
    "    }\n",
    "\n",
    "ground_eval = evaluate_grounding_on_custom_lyrics(test_lyrics, df)\n",
    "print(ground_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP-grounding rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_shap_alignment(df, num_samples=40):\n",
    "    rng = np.random.default_rng(2)\n",
    "    idxs = rng.choice(len(df), size=num_samples, replace=False)\n",
    "\n",
    "    aligned = 0\n",
    "\n",
    "    for idx in idxs:\n",
    "        lyric = df.iloc[idx][\"lyrics\"]\n",
    "\n",
    "        # Run pipeline\n",
    "        query_embedding = embed_lyrics(lyric)\n",
    "        neighbors = get_top_k_neighbors(df, query_embedding, 3)\n",
    "        x_vec = construct_feature_vector(query_embedding, neighbors, audio_feature_cols, 3)\n",
    "        shap_vals = explainer.shap_values(x_vec.reshape(1, -1))\n",
    "        \n",
    "        EMB_DIM = len(query_embedding)\n",
    "        shap_grouped = group_shap_fully(shap_vals, EMB_DIM, audio_feature_cols, 3)\n",
    "\n",
    "        explanation = rag_lyric_popularity_system(df, lyric)[\"explanation\"].lower()\n",
    "\n",
    "        # Check if major contributors appear in explanation text\n",
    "        main_terms = []\n",
    "        if abs(shap_grouped[\"target_embedding\"]) > 0.1:\n",
    "            main_terms.append(\"lyric\")\n",
    "        for i, nb in enumerate(shap_grouped[\"neighbors\"]):\n",
    "            if abs(nb[\"similarity\"]) > 0.1:\n",
    "                main_terms.append(\"similarity\")\n",
    "            if abs(nb[\"popularity\"]) > 0.1:\n",
    "                main_terms.append(\"popular\")\n",
    "\n",
    "        if any(term in explanation for term in main_terms):\n",
    "            aligned += 1\n",
    "\n",
    "    return aligned / num_samples\n",
    "\n",
    "alignment_rate = evaluate_shap_alignment(df)\n",
    "print(\"SHAP-grounding rate:\", alignment_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across our evaluation suite, the results demonstrate both strengths and key limitations of the current RAG explanation pipeline. The JSON validity evaluation achieved a 100% valid_json_rate, indicating that the LLM consistently produced clean, machine-parseable JSON without requiring repair—an essential property for reliable downstream automation. The evidence-grounding analysis also returned a 1.0 grounding rate, showing that every explanation referenced at least one of the FAISS-retrieved neighbors as intended. We test this with lyrics in our dataset plus some of our own thought lyrics. Finally, the SHAP-alignment test achieved a perfect 1.0 alignment rate, confirming that the LLM faithfully incorporates the model’s true attributions like neighbor similarity and embedding influence into its narrative explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse JSON from LLM output. Returning raw text.\n",
      "{'mean_coverage': 0.8888888888888888, 'coverage_per_lyric': [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666]}\n"
     ]
    }
   ],
   "source": [
    "def compute_explanation_coverage(test_lyrics, df, k_neighbors=3):\n",
    "    \"\"\"\n",
    "    Measures how many retrieved neighbors are actually referenced in the LLM explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    coverage_scores = []\n",
    "\n",
    "    for lyric in test_lyrics:\n",
    "        out = rag_lyric_popularity_system(df, lyric, k_neighbors=k_neighbors)\n",
    "\n",
    "        explanation = out[\"explanation\"].lower()\n",
    "        neighbors = out[\"neighbors_used\"]\n",
    "\n",
    "        titles = [nb[\"title\"].lower() for nb in neighbors]\n",
    "        artists = [nb[\"artist\"].lower() for nb in neighbors]\n",
    "\n",
    "        count = 0\n",
    "        for t, a in zip(titles, artists):\n",
    "            if t in explanation or a in explanation:\n",
    "                count += 1\n",
    "\n",
    "        coverage_scores.append(count / k_neighbors)\n",
    "\n",
    "    return {\n",
    "        \"mean_coverage\": float(np.mean(coverage_scores)),\n",
    "        \"coverage_per_lyric\": coverage_scores\n",
    "    }\n",
    "print(compute_explanation_coverage(test_lyrics, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coverage function checks how many of the retrieved neighbors appear explicitly in the LLM-generated explanation. This measures whether the RAG explanation actually uses the retrieved evidence instead of generating generic text. Mean coverage score of 0.89 shows that the LLM references most neighbors in its explanations, confirming that explanations are strongly grounded in the retrieved examples as intended. Compred to our test of lyrics in our dataset, it seems that shorter lyrics confused the LLM more. When prompts are short, the model may by:\n",
    "\n",
    "- Struggle to ground itself\n",
    "\n",
    "- Produce fallback text\n",
    "\n",
    "- Skip JSON formatting\n",
    "\n",
    "- Enter a non-instruction-following mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
